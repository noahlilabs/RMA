# Infini-Attention GPU Demo

A GPU-accelerated implementation of Attention mechanism for efficiently processing long documents with constant memory usage.

## What Is Infinite Attention?

Infinite Attention is a novel attention mechanism that solves the quadratic complexity problem of standard attention by:
- Processing text in fixed-size segments
- Maintaining persistent memory matrices on the GPU
- Using a gating mechanism to balance local and global context

This enables processing documents of arbitrary length while maintaining context across segments.

## Features

- üöÄ **GPU-Accelerated**: Uses WGPU for cross-platform GPU acceleration
- üìù **Format Support**: Processes TXT, PDF, and DOCX documents
- üß† **Memory Efficient**: Constant memory usage regardless of document length
- üîÑ **Streaming**: Processes documents as streams rather than loading entirely into memory
- üßÆ **Multi-Head Attention**: Supports multiple attention heads for better representation

## Prerequisites

- **Rust and Cargo** - Install via `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`
- **PDF Support** (optional) - Install poppler: 
  - macOS: `brew install poppler`
  - Linux: `sudo apt-get install poppler-utils`
- **DOCX Support** (optional) - Install pandoc:
  - macOS: `brew install pandoc`
  - Linux: `sudo apt-get install pandoc`

## Building and Running

```bash
# Build
cargo build --release

# Run
./target/release/infini_attention_gpu_demo --input "path/to/file.pdf" --segment-size 16 --embed-dim 12 --gpu
```

## Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--input, -i` | Input file path (PDF, TXT, or DOCX) | Required |
| `--segment-size, -s` | Size of attention segments | 16 |
| `--embed-dim, -e` | Embedding dimension (must be divisible by 3) | 12 |
| `--vocab-size, -v` | Vocabulary size | 10000 |
| `--heads` | Number of attention heads | 1 |
| `--gpu` | Enable GPU acceleration | false |

## How It Works

1. **Document Processing**: Reads and converts documents to text streams
2. **Tokenization**: Converts text to token IDs
3. **Segmentation**: Groups tokens into fixed-size segments
4. **Embedding**: Maps tokens to vector embeddings
5. **Attention Computation**: Processes each segment through the infinite attention mechanism
6. **Memory Update**: Updates persistent memory matrices after each segment

## Technical Notes

- The `embed-dim` must be divisible by 3 for the Q/K/V split in attention computation
- For processing PDFs, make sure poppler-utils is installed
- For DOCX files, pandoc must be installed
- Uses WGPU for GPU operations - no other GPU libraries required